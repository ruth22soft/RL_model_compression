{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOi4WysgNAv5KHDjtnHSyfs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ruth22soft/RL_model_compression/blob/main/Cartpole_model_compression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "awNhDy4DcFLH",
        "outputId": "cfb914be-1b3b-4cfe-87f1-061de72a5f00"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gym in /usr/local/lib/python3.12/dist-packages (0.25.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cpu)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from gym) (3.1.2)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.12/dist-packages (from gym) (0.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.61.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n"
          ]
        }
      ],
      "source": [
        "pip install gym torch numpy matplotlib\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install gymnasium\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fvueXXI5l0nd",
        "outputId": "32a460f3-18ed-49de-ed48-cc557b95ab55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gymnasium in /usr/local/lib/python3.12/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium) (3.1.2)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium) (4.15.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from gymnasium) (0.0.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.utils.prune as prune\n",
        "import random\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "import time\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# --- DQN Model ---\n",
        "class DQN(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        super(DQN, self).__init__()\n",
        "        self.fc1 = nn.Linear(state_dim, 128)\n",
        "        self.fc2 = nn.Linear(128, 128)\n",
        "        self.out = nn.Linear(128, action_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        return self.out(x)\n",
        "\n",
        "# --- Replay Buffer ---\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity):\n",
        "        self.buffer = deque(maxlen=capacity)\n",
        "\n",
        "    def push(self, state, action, reward, next_state, done):\n",
        "        self.buffer.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        batch = random.sample(self.buffer, batch_size)\n",
        "        states, actions, rewards, next_states, dones = zip(*batch)\n",
        "        return np.array(states), actions, rewards, np.array(next_states), dones\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "# --- Training ---\n",
        "def train_dqn(env, episodes=300):\n",
        "    state_dim = env.observation_space.shape[0]\n",
        "    action_dim = env.action_space.n\n",
        "    model = DQN(state_dim, action_dim)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "    criterion = nn.MSELoss()\n",
        "    buffer = ReplayBuffer(10000)\n",
        "    gamma = 0.99\n",
        "    batch_size = 64\n",
        "    epsilon = 1.0\n",
        "    epsilon_decay = 0.995\n",
        "    epsilon_min = 0.01\n",
        "\n",
        "    rewards_history = []\n",
        "\n",
        "    for ep in range(episodes):\n",
        "        state, _ = env.reset()\n",
        "        done = False\n",
        "        total_reward = 0\n",
        "\n",
        "        while not done:\n",
        "            # --- Epsilon-greedy ---\n",
        "            if random.random() < epsilon:\n",
        "                action = env.action_space.sample()\n",
        "            else:\n",
        "                with torch.no_grad():\n",
        "                    # Fix: Add unsqueeze(0) to add a batch dimension\n",
        "                    action = torch.argmax(model(torch.tensor(state, dtype=torch.float32).unsqueeze(0))).item()\n",
        "\n",
        "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "            done = terminated or truncated\n",
        "            buffer.push(state, action, reward, next_state, done)\n",
        "            state = next_state\n",
        "            total_reward += reward\n",
        "\n",
        "            # --- Update DQN ---\n",
        "            if len(buffer) > batch_size:\n",
        "                states, actions, rewards, next_states, dones = buffer.sample(batch_size)\n",
        "                states = torch.tensor(states, dtype=torch.float32)\n",
        "                next_states = torch.tensor(next_states, dtype=torch.float32)\n",
        "                actions = torch.tensor(actions)\n",
        "                rewards = torch.tensor(rewards, dtype=torch.float32)\n",
        "                dones = torch.tensor(dones, dtype=torch.float32)\n",
        "\n",
        "                q_values = model(states).gather(1, actions.unsqueeze(1)).squeeze()\n",
        "                next_q_values = model(next_states).max(1)[0]\n",
        "                target = rewards + gamma * next_q_values * (1 - dones)\n",
        "\n",
        "                loss = criterion(q_values, target)\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "        rewards_history.append(total_reward)\n",
        "        epsilon = max(epsilon * epsilon_decay, epsilon_min)\n",
        "\n",
        "        if (ep+1) % 20 == 0:\n",
        "            print(f\"Episode {ep+1}, Reward: {total_reward}, Epsilon: {epsilon:.2f}\")\n",
        "\n",
        "    return model, rewards_history\n",
        "\n",
        "# --- Apply Pruning ---\n",
        "def apply_pruning(model, amount=0.3):\n",
        "    for layer in [model.fc1, model.fc2, model.out]:\n",
        "        prune.l1_unstructured(layer, name=\"weight\", amount=amount)\n",
        "        prune.remove(layer, 'weight')  # make pruning permanent\n",
        "\n",
        "# --- Evaluate Model ---\n",
        "def evaluate_model(env, model, episodes=50):\n",
        "    device = torch.device(\"cpu\")\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    total_rewards = []\n",
        "    inference_times = []\n",
        "\n",
        "    for _ in range(episodes):\n",
        "        state, _ = env.reset()\n",
        "        done = False\n",
        "        ep_reward = 0\n",
        "\n",
        "        while not done:\n",
        "            start = time.time()\n",
        "            with torch.no_grad():\n",
        "                # Fix: Add unsqueeze(0) to add a batch dimension\n",
        "                action = torch.argmax(model(torch.tensor(state, dtype=torch.float32).to(device).unsqueeze(0))).item()\n",
        "            end = time.time()\n",
        "            inference_times.append(end - start)\n",
        "\n",
        "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "            done = terminated or truncated\n",
        "            state = next_state\n",
        "            ep_reward += reward\n",
        "\n",
        "        total_rewards.append(ep_reward)\n",
        "\n",
        "    avg_reward = np.mean(total_rewards)\n",
        "    avg_inference_time = np.mean(inference_times)\n",
        "\n",
        "    # Model size in MB\n",
        "    model_file = \"temp_model.pth\"\n",
        "    # Ensure model is on CPU before saving state_dict for size calculation\n",
        "    model_cpu = model.to(\"cpu\")\n",
        "    torch.save(model_cpu.state_dict(), model_file)\n",
        "    model_size = os.path.getsize(model_file) / (1024 * 1024)\n",
        "    os.remove(model_file)\n",
        "\n",
        "    return avg_reward, avg_inference_time, model_size\n",
        "\n",
        "# --- Main ---\n",
        "env = gym.make(\"CartPole-v1\")\n",
        "\n",
        "# 1️⃣ Train baseline model\n",
        "baseline_model, rewards = train_dqn(env)\n",
        "baseline_avg_reward, baseline_inference_time, baseline_size_mb = evaluate_model(env, baseline_model)\n",
        "print(f\"\\n--- Baseline ---\\nAverage Reward: {baseline_avg_reward:.2f}\\nInference Time per Step: {baseline_inference_time*1000:.4f} ms\\nModel Size: {baseline_size_mb:.4f} MB\")\n",
        "\n",
        "# 2️⃣ Apply pruning\n",
        "apply_pruning(baseline_model, amount=0.3)\n",
        "\n",
        "# 3️⃣ Apply dynamic quantization\n",
        "quantized_model = torch.quantization.quantize_dynamic(\n",
        "    baseline_model, {nn.Linear}, dtype=torch.qint8\n",
        ")\n",
        "\n",
        "# 4️⃣ Evaluate quantized + pruned model\n",
        "quant_avg_reward, quant_inference_time, quant_size_mb = evaluate_model(env, quantized_model)\n",
        "print(f\"\\n--- Pruned + Quantized ---\\nAverage Reward: {quant_avg_reward:.2f}\\nInference Time per Step: {quant_inference_time*1000:.4f} ms\\nModel Size: {quant_size_mb:.4f} MB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v3Lw1-K-ykJp",
        "outputId": "b810cb80-d450-4081-a5c8-3311c5e19ab6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 20, Reward: 17.0, Epsilon: 0.90\n",
            "Episode 40, Reward: 56.0, Epsilon: 0.82\n",
            "Episode 60, Reward: 45.0, Epsilon: 0.74\n",
            "Episode 80, Reward: 20.0, Epsilon: 0.67\n",
            "Episode 100, Reward: 78.0, Epsilon: 0.61\n",
            "Episode 120, Reward: 63.0, Epsilon: 0.55\n",
            "Episode 140, Reward: 84.0, Epsilon: 0.50\n",
            "Episode 160, Reward: 47.0, Epsilon: 0.45\n",
            "Episode 180, Reward: 251.0, Epsilon: 0.41\n",
            "Episode 200, Reward: 81.0, Epsilon: 0.37\n",
            "Episode 220, Reward: 81.0, Epsilon: 0.33\n",
            "Episode 240, Reward: 91.0, Epsilon: 0.30\n",
            "Episode 260, Reward: 129.0, Epsilon: 0.27\n",
            "Episode 280, Reward: 70.0, Epsilon: 0.25\n",
            "Episode 300, Reward: 310.0, Epsilon: 0.22\n",
            "\n",
            "--- Baseline ---\n",
            "Average Reward: 382.82\n",
            "Inference Time per Step: 0.1290 ms\n",
            "Model Size: 0.0693 MB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-318911287.py:164: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
            "For migrations of users: \n",
            "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
            "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
            "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
            "see https://github.com/pytorch/ao/issues/2259 for more details\n",
            "  quantized_model = torch.quantization.quantize_dynamic(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/ao/quantization/quantize.py:566: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
            "For migrations of users: \n",
            "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
            "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
            "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
            "see https://github.com/pytorch/ao/issues/2259 for more details\n",
            "  convert(model, mapping, inplace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Pruned + Quantized ---\n",
            "Average Reward: 18.96\n",
            "Inference Time per Step: 0.2937 ms\n",
            "Model Size: 0.0221 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary dependencies for Box2D environments\n",
        "!pip install swig\n",
        "!pip install \"gymnasium[box2d]\"\n",
        "\n",
        "import gymnasium as gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import random\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "import time\n",
        "import os\n",
        "\n",
        "# --- DQN Model ---\n",
        "class DQN(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        super(DQN, self).__init__()\n",
        "        self.fc1 = nn.Linear(state_dim, 256)\n",
        "        self.fc2 = nn.Linear(256, 256)\n",
        "        self.out = nn.Linear(256, action_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        return self.out(x)\n",
        "\n",
        "# --- Replay Buffer ---\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity):\n",
        "        self.buffer = deque(maxlen=capacity)\n",
        "\n",
        "    def push(self, state, action, reward, next_state, done):\n",
        "        self.buffer.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        batch = random.sample(self.buffer, batch_size)\n",
        "        states, actions, rewards, next_states, dones = zip(*batch)\n",
        "        return np.array(states), actions, rewards, np.array(next_states), dones\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "# --- Training Function ---\n",
        "def train_dqn(env, episodes=300):\n",
        "    state_dim = env.observation_space.shape[0]\n",
        "    action_dim = env.action_space.n\n",
        "    model = DQN(state_dim, action_dim)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "    criterion = nn.MSELoss()\n",
        "    buffer = ReplayBuffer(10000)\n",
        "    gamma = 0.99\n",
        "    batch_size = 64\n",
        "    epsilon = 1.0\n",
        "    epsilon_decay = 0.995\n",
        "    epsilon_min = 0.01\n",
        "\n",
        "    rewards_history = []\n",
        "\n",
        "    for ep in range(episodes):\n",
        "        state, _ = env.reset()\n",
        "        done = False\n",
        "        total_reward = 0\n",
        "\n",
        "        while not done:\n",
        "            if random.random() < epsilon:\n",
        "                action = env.action_space.sample()\n",
        "            else:\n",
        "                with torch.no_grad():\n",
        "                    action = torch.argmax(model(torch.tensor(state, dtype=torch.float32))).item()\n",
        "\n",
        "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "            done = terminated or truncated\n",
        "            buffer.push(state, action, reward, next_state, done)\n",
        "            state = next_state\n",
        "            total_reward += reward\n",
        "\n",
        "            if len(buffer) > batch_size:\n",
        "                states, actions, rewards, next_states, dones = buffer.sample(batch_size)\n",
        "                states = torch.tensor(states, dtype=torch.float32)\n",
        "                next_states = torch.tensor(next_states, dtype=torch.float32)\n",
        "                actions = torch.tensor(actions)\n",
        "                rewards = torch.tensor(rewards, dtype=torch.float32)\n",
        "                dones = torch.tensor(dones, dtype=torch.float32)\n",
        "\n",
        "                q_values = model(states).gather(1, actions.unsqueeze(1)).squeeze()\n",
        "                next_q_values = model(next_states).max(1)[0]\n",
        "                target = rewards + gamma * next_q_values * (1 - dones)\n",
        "\n",
        "                loss = criterion(q_values, target)\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "        rewards_history.append(total_reward)\n",
        "        epsilon = max(epsilon * epsilon_decay, epsilon_min)\n",
        "        if (ep+1) % 20 == 0:\n",
        "            print(f\"Episode {ep+1}, Reward: {total_reward}, Epsilon: {epsilon:.2f}\")\n",
        "\n",
        "    return model, rewards_history\n",
        "\n",
        "# --- Evaluation: model size, inference time, average reward ---\n",
        "def evaluate_model(env, model, episodes=50):\n",
        "    device = torch.device(\"cpu\")\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    total_rewards = []\n",
        "    inference_times = []\n",
        "\n",
        "    for ep in range(episodes):\n",
        "        state, _ = env.reset()\n",
        "        done = False\n",
        "        ep_reward = 0\n",
        "\n",
        "        while not done:\n",
        "            start = time.time()\n",
        "            with torch.no_grad():\n",
        "                action = torch.argmax(model(torch.tensor(state, dtype=torch.float32).to(device))).item()\n",
        "            end = time.time()\n",
        "            inference_times.append(end - start)\n",
        "\n",
        "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "            done = terminated or truncated\n",
        "            state = next_state\n",
        "            ep_reward += reward\n",
        "\n",
        "        total_rewards.append(ep_reward)\n",
        "\n",
        "    avg_reward = np.mean(total_rewards)\n",
        "    avg_inference_time = np.mean(inference_times)\n",
        "\n",
        "    # Model size in MB\n",
        "    model_file = \"temp_model.pth\"\n",
        "    torch.save(model.state_dict(), model_file)\n",
        "    model_size = os.path.getsize(model_file) / (1024 * 1024)\n",
        "    os.remove(model_file)\n",
        "\n",
        "    return avg_reward, avg_inference_time, model_size\n",
        "\n",
        "# --- Main ---\n",
        "if __name__ == \"__main__\":\n",
        "    env = gym.make(\"LunarLander-v3\")\n",
        "    model, rewards_history = train_dqn(env, episodes=300)\n",
        "\n",
        "    avg_reward, avg_inference_time, model_size = evaluate_model(env, model, episodes=50)\n",
        "    print(f\"\\n--- Baseline Evaluation ---\")\n",
        "    print(f\"Average Reward: {avg_reward:.2f}\")\n",
        "    print(f\"Average Inference Time per Step: {avg_inference_time*1000:.4f} ms\")\n",
        "    print(f\"Model Size: {model_size:.4f} MB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QSonYF_wyjzb",
        "outputId": "f8446d13-fcde-47a7-9c4b-12ea424b352a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting swig\n",
            "  Using cached swig-4.4.0-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (3.5 kB)\n",
            "Using cached swig-4.4.0-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.9 MB)\n",
            "Installing collected packages: swig\n",
            "Successfully installed swig-4.4.0\n",
            "Requirement already satisfied: gymnasium[box2d] in /usr/local/lib/python3.12/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium[box2d]) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium[box2d]) (3.1.2)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium[box2d]) (4.15.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from gymnasium[box2d]) (0.0.4)\n",
            "Collecting box2d-py==2.3.5 (from gymnasium[box2d])\n",
            "  Using cached box2d-py-2.3.5.tar.gz (374 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.12/dist-packages (from gymnasium[box2d]) (2.6.1)\n",
            "Requirement already satisfied: swig==4.* in /usr/local/lib/python3.12/dist-packages (from gymnasium[box2d]) (4.4.0)\n",
            "Building wheels for collected packages: box2d-py\n",
            "  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for box2d-py: filename=box2d_py-2.3.5-cp312-cp312-linux_x86_64.whl size=2399003 sha256=59510676bd7f82661f818d983dddc36f8257ccec41185c697b72c9fcc4e6e1ee\n",
            "  Stored in directory: /root/.cache/pip/wheels/2a/e9/60/774da0bcd07f7dc7761a8590fa2d065e4069568e78dcdc3318\n",
            "Successfully built box2d-py\n",
            "Installing collected packages: box2d-py\n",
            "Successfully installed box2d-py-2.3.5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/pygame/pkgdata.py:25: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
            "  from pkg_resources import resource_stream, resource_exists\n",
            "/usr/local/lib/python3.12/dist-packages/pkg_resources/__init__.py:3154: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.\n",
            "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
            "  declare_namespace(pkg)\n",
            "/usr/local/lib/python3.12/dist-packages/pkg_resources/__init__.py:3154: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google.cloud')`.\n",
            "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
            "  declare_namespace(pkg)\n",
            "/usr/local/lib/python3.12/dist-packages/pkg_resources/__init__.py:3154: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('sphinxcontrib')`.\n",
            "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
            "  declare_namespace(pkg)\n",
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 20, Reward: -146.82355593685656, Epsilon: 0.90\n",
            "Episode 40, Reward: -90.54508252688015, Epsilon: 0.82\n",
            "Episode 60, Reward: -26.813587929539338, Epsilon: 0.74\n",
            "Episode 80, Reward: -3.8993270585102664, Epsilon: 0.67\n",
            "Episode 100, Reward: -53.76979022028717, Epsilon: 0.61\n",
            "Episode 120, Reward: -90.24706358136815, Epsilon: 0.55\n",
            "Episode 140, Reward: -20.02237741367246, Epsilon: 0.50\n",
            "Episode 160, Reward: -16.89932916219827, Epsilon: 0.45\n",
            "Episode 180, Reward: -58.62099605737181, Epsilon: 0.41\n",
            "Episode 200, Reward: -127.75485764092473, Epsilon: 0.37\n",
            "Episode 220, Reward: 12.53084152829546, Epsilon: 0.33\n",
            "Episode 240, Reward: 33.27659252822602, Epsilon: 0.30\n",
            "Episode 260, Reward: -39.94565030067594, Epsilon: 0.27\n",
            "Episode 280, Reward: -115.63799953769578, Epsilon: 0.25\n",
            "Episode 300, Reward: -69.78464827939963, Epsilon: 0.22\n",
            "\n",
            "--- Baseline Evaluation ---\n",
            "Average Reward: -52.75\n",
            "Average Inference Time per Step: 0.1354 ms\n",
            "Model Size: 0.2666 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import random\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "import time\n",
        "import os\n",
        "import torch.nn.utils.prune as prune\n",
        "import copy\n",
        "\n",
        "# --- DQN Model ---\n",
        "class DQN(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        super(DQN, self).__init__()\n",
        "        self.fc1 = nn.Linear(state_dim, 256)\n",
        "        self.fc2 = nn.Linear(256, 256)\n",
        "        self.out = nn.Linear(256, action_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        return self.out(x)\n",
        "\n",
        "# --- Replay Buffer ---\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity):\n",
        "        self.buffer = deque(maxlen=capacity)\n",
        "\n",
        "    def push(self, state, action, reward, next_state, done):\n",
        "        self.buffer.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        batch = random.sample(self.buffer, batch_size)\n",
        "        states, actions, rewards, next_states, dones = zip(*batch)\n",
        "        return np.array(states), actions, rewards, np.array(next_states), dones\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "# --- Training Function ---\n",
        "def train_dqn(env, episodes=300):\n",
        "    state_dim = env.observation_space.shape[0]\n",
        "    action_dim = env.action_space.n\n",
        "    model = DQN(state_dim, action_dim)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "    criterion = nn.MSELoss()\n",
        "    buffer = ReplayBuffer(10000)\n",
        "    gamma = 0.99\n",
        "    batch_size = 64\n",
        "    epsilon = 1.0\n",
        "    epsilon_decay = 0.995\n",
        "    epsilon_min = 0.01\n",
        "\n",
        "    rewards_history = []\n",
        "\n",
        "    for ep in range(episodes):\n",
        "        state, _ = env.reset()\n",
        "        done = False\n",
        "        total_reward = 0\n",
        "\n",
        "        while not done:\n",
        "            if random.random() < epsilon:\n",
        "                action = env.action_space.sample()\n",
        "            else:\n",
        "                with torch.no_grad():\n",
        "                    action = torch.argmax(model(torch.tensor(state, dtype=torch.float32).unsqueeze(0))).item()\n",
        "\n",
        "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "            done = terminated or truncated\n",
        "            buffer.push(state, action, reward, next_state, done)\n",
        "            state = next_state\n",
        "            total_reward += reward\n",
        "\n",
        "            if len(buffer) > batch_size:\n",
        "                states, actions, rewards, next_states, dones = buffer.sample(batch_size)\n",
        "                states = torch.tensor(states, dtype=torch.float32)\n",
        "                next_states = torch.tensor(next_states, dtype=torch.float32)\n",
        "                actions = torch.tensor(actions)\n",
        "                rewards = torch.tensor(rewards, dtype=torch.float32)\n",
        "                dones = torch.tensor(dones, dtype=torch.float32)\n",
        "\n",
        "                q_values = model(states).gather(1, actions.unsqueeze(1)).squeeze()\n",
        "                next_q_values = model(next_states).max(1)[0]\n",
        "                target = rewards + gamma * next_q_values * (1 - dones)\n",
        "\n",
        "                loss = criterion(q_values, target)\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "        rewards_history.append(total_reward)\n",
        "        epsilon = max(epsilon * epsilon_decay, epsilon_min)\n",
        "        if (ep+1) % 20 == 0:\n",
        "            print(f\"Episode {ep+1}, Reward: {total_reward:.2f}, Epsilon: {epsilon:.2f}\")\n",
        "\n",
        "    return model, rewards_history\n",
        "\n",
        "# --- Evaluation: model size, inference time, average reward, and accuracy ---\n",
        "def evaluate_model(env, model, episodes=50, success_threshold=200):\n",
        "    device = torch.device(\"cpu\")\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    total_rewards = []\n",
        "    inference_times = []\n",
        "    successful_episodes = 0\n",
        "\n",
        "    for ep in range(episodes):\n",
        "        state, _ = env.reset()\n",
        "        done = False\n",
        "        ep_reward = 0\n",
        "\n",
        "        while not done:\n",
        "            start = time.time()\n",
        "            with torch.no_grad():\n",
        "                action = torch.argmax(model(torch.tensor(state, dtype=torch.float32).to(device).unsqueeze(0))).item()\n",
        "            end = time.time()\n",
        "            inference_times.append(end - start)\n",
        "\n",
        "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "            done = terminated or truncated\n",
        "            state = next_state\n",
        "            ep_reward += reward\n",
        "\n",
        "        total_rewards.append(ep_reward)\n",
        "        if ep_reward >= success_threshold:\n",
        "            successful_episodes += 1\n",
        "\n",
        "    avg_reward = np.mean(total_rewards)\n",
        "    avg_inference_time = np.mean(inference_times)\n",
        "    accuracy = (successful_episodes / episodes) * 100 if episodes > 0 else 0\n",
        "\n",
        "    # Model size in MB\n",
        "    model_file = \"temp_model.pth\"\n",
        "    torch.save(model.state_dict(), model_file)\n",
        "    model_size = os.path.getsize(model_file) / (1024 * 1024)\n",
        "    os.remove(model_file)\n",
        "\n",
        "    return avg_reward, avg_inference_time, model_size, accuracy\n",
        "\n",
        "# --- Compression Techniques ---\n",
        "def apply_pruning(model, amount=0.3):\n",
        "    \"\"\"\n",
        "    Prune 'amount' of weights in each linear layer (unstructured pruning)\n",
        "    \"\"\"\n",
        "    for name, module in model.named_modules():\n",
        "        if isinstance(module, nn.Linear):\n",
        "            prune.l1_unstructured(module, name='weight', amount=amount)\n",
        "            prune.remove(module, 'weight') # Make pruning permanent\n",
        "    return model\n",
        "\n",
        "def apply_quantization(model):\n",
        "    \"\"\"\n",
        "    Convert model to 8-bit integers (dynamic quantization)\n",
        "    \"\"\"\n",
        "    # Ensure model is in eval mode before quantization\n",
        "    model.eval()\n",
        "    model_quantized = torch.quantization.quantize_dynamic(\n",
        "        model, {nn.Linear}, dtype=torch.qint8\n",
        "    )\n",
        "    return model_quantized\n",
        "\n",
        "# --- Main ---\n",
        "if __name__ == \"__main__\":\n",
        "    env = gym.make(\"LunarLander-v3\")\n",
        "\n",
        "    print(\"\\n--- Training Base Model ---\")\n",
        "    base_model, rewards_history = train_dqn(env, episodes=300)\n",
        "\n",
        "    print(\"\\n--- Evaluating Base Model ---\")\n",
        "    base_reward, base_time, base_size, base_accuracy = evaluate_model(env, base_model, episodes=50)\n",
        "    print(f\"Base Model -> Reward: {base_reward:.2f}, Accuracy: {base_accuracy:.2f}%, Inference: {base_time*1000:.4f} ms, Size: {base_size:.4f} MB\")\n",
        "\n",
        "    # --- Apply Pruning ---\n",
        "    print(\"\\n--- Applying Pruning (30%) ---\")\n",
        "    # Create a deep copy of the base model before pruning to keep the original for other operations\n",
        "    pruning_model = copy.deepcopy(base_model)\n",
        "    pruned_model = apply_pruning(pruning_model, amount=0.3)\n",
        "    pruned_reward, pruned_time, pruned_size, pruned_accuracy = evaluate_model(env, pruned_model, episodes=50)\n",
        "    print(f\"Pruned Model -> Reward: {pruned_reward:.2f}, Accuracy: {pruned_accuracy:.2f}%, Inference: {pruned_time*1000:.4f} ms, Size: {pruned_size:.4f} MB\")\n",
        "\n",
        "    # --- Apply Quantization ---\n",
        "    print(\"\\n--- Applying Quantization ---\")\n",
        "    # Create a deep copy of the base model before quantization\n",
        "    quantization_model = copy.deepcopy(base_model)\n",
        "    quantized_model = apply_quantization(quantization_model)\n",
        "    quant_reward, quant_time, quant_size, quant_accuracy = evaluate_model(env, quantized_model, episodes=50)\n",
        "    print(f\"Quantized Model -> Reward: {quant_reward:.2f}, Accuracy: {quant_accuracy:.2f}%, Inference: {quant_time*1000:.4f} ms, Size: {quant_size:.4f} MB\")\n",
        "\n",
        "    # --- Apply Pruning + Quantization ---\n",
        "    print(\"\\n--- Applying Pruning + Quantization ---\")\n",
        "    # Create a deep copy of the base model, then prune it, then quantize the pruned model\n",
        "    pruned_quant_base = copy.deepcopy(base_model)\n",
        "    pruned_then_quant_model = apply_pruning(pruned_quant_base, amount=0.3)\n",
        "    pruned_then_quant_model = apply_quantization(pruned_then_quant_model)\n",
        "    pq_reward, pq_time, pq_size, pq_accuracy = evaluate_model(env, pruned_then_quant_model, episodes=50)\n",
        "    print(f\"Pruned+Quantized Model -> Reward: {pq_reward:.2f}, Accuracy: {pq_accuracy:.2f}%, Inference: {pq_time*1000:.4f} ms, Size: {pq_size:.4f} MB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f5ujEGjlClma",
        "outputId": "d160bf2b-eb5e-4357-ec5e-82060105f0d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Training Base Model ---\n",
            "Episode 20, Reward: -121.42, Epsilon: 0.90\n",
            "Episode 40, Reward: -175.49, Epsilon: 0.82\n",
            "Episode 60, Reward: -98.90, Epsilon: 0.74\n",
            "Episode 80, Reward: -80.10, Epsilon: 0.67\n",
            "Episode 100, Reward: -96.96, Epsilon: 0.61\n",
            "Episode 120, Reward: -55.43, Epsilon: 0.55\n",
            "Episode 140, Reward: -45.16, Epsilon: 0.50\n",
            "Episode 160, Reward: -72.01, Epsilon: 0.45\n",
            "Episode 180, Reward: -42.45, Epsilon: 0.41\n",
            "Episode 200, Reward: -25.54, Epsilon: 0.37\n",
            "Episode 220, Reward: 33.45, Epsilon: 0.33\n",
            "Episode 240, Reward: 1.91, Epsilon: 0.30\n",
            "Episode 260, Reward: 170.40, Epsilon: 0.27\n",
            "Episode 280, Reward: -7.42, Epsilon: 0.25\n",
            "Episode 300, Reward: -7.36, Epsilon: 0.22\n",
            "\n",
            "--- Evaluating Base Model ---\n",
            "Base Model -> Reward: 100.33, Accuracy: 42.00%, Inference: 0.1491 ms, Size: 0.2666 MB\n",
            "\n",
            "--- Applying Pruning (30%) ---\n",
            "Pruned Model -> Reward: 67.94, Accuracy: 42.00%, Inference: 0.1287 ms, Size: 0.2666 MB\n",
            "\n",
            "--- Applying Quantization ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3404985446.py:160: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
            "For migrations of users: \n",
            "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
            "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
            "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
            "see https://github.com/pytorch/ao/issues/2259 for more details\n",
            "  model_quantized = torch.quantization.quantize_dynamic(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/ao/quantization/quantize.py:566: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
            "For migrations of users: \n",
            "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
            "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
            "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
            "see https://github.com/pytorch/ao/issues/2259 for more details\n",
            "  convert(model, mapping, inplace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Quantized Model -> Reward: 109.70, Accuracy: 50.00%, Inference: 0.3304 ms, Size: 0.0721 MB\n",
            "\n",
            "--- Applying Pruning + Quantization ---\n",
            "Pruned+Quantized Model -> Reward: 25.41, Accuracy: 28.00%, Inference: 0.3108 ms, Size: 0.0721 MB\n"
          ]
        }
      ]
    }
  ]
}
